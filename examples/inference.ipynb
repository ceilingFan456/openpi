{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/t-qimhuang/miniconda3/envs/openpi/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import dataclasses\n",
                "\n",
                "import jax\n",
                "\n",
                "from openpi.models import model as _model\n",
                "from openpi.policies import droid_policy\n",
                "from openpi.policies import policy_config as _policy_config\n",
                "from openpi.shared import download\n",
                "from openpi.training import config as _config\n",
                "from openpi.training import data_loader as _data_loader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Policy inference\n",
                "\n",
                "The following example shows how to create a policy from a checkpoint and run inference on a dummy example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Example keys: ['observation/exterior_image_1_left', 'observation/wrist_image_left', 'observation/joint_position', 'observation/gripper_position', 'prompt']\n",
                        "Example shapes: {'observation/exterior_image_1_left': <class 'numpy.ndarray'>, 'observation/wrist_image_left': <class 'numpy.ndarray'>, 'observation/joint_position': <class 'numpy.ndarray'>, 'observation/gripper_position': <class 'numpy.ndarray'>, 'prompt': <class 'str'>}\n",
                        "Result keys: ['actions', 'policy_timing']\n",
                        "Result shapes: {'actions': <class 'numpy.ndarray'>, 'policy_timing': <class 'dict'>}\n",
                        "Actions shape: (10, 8)\n"
                    ]
                }
            ],
            "source": [
                "# config = _config.get_config(\"pi0_fast_droid\")\n",
                "# checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_fast_droid\")\n",
                "\n",
                "from pathlib import Path\n",
                "config = _config.get_config(\"pi0_fast_droid\")\n",
                "checkpoint_dir = Path(\"/home/t-qimhuang/code/openpi/checkpoints//pi0_fast_droid\")\n",
                "\n",
                "# from pathlib import Path\n",
                "# config = _config.get_config(\"pi05_libero\")\n",
                "# checkpoint_dir = Path(\"/home/t-qimhuang/code/openpi/checkpoints/pi05_libero\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
                "\n",
                "# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\n",
                "example = droid_policy.make_droid_example()\n",
                "print(\"Example keys:\", list(example.keys()))\n",
                "\n",
                "print(\"Example shapes:\", {k: type(v) for k, v in example.items()})\n",
                "\n",
                "result = policy.infer(example)\n",
                "\n",
                "print(\"Result keys:\", list(result.keys()))\n",
                "print(\"Result shapes:\", {k: type(v) for k, v in result.items()})\n",
                "\n",
                "# Delete the policy to free up memory.\n",
                "del policy\n",
                "\n",
                "print(\"Actions shape:\", result[\"actions\"].shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Working with a live model\n",
                "\n",
                "\n",
                "The following example shows how to create a live model from a checkpoint and compute training loss. First, we are going to demonstrate how to do it with fake data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loss shape: (1, 50)\n"
                    ]
                }
            ],
            "source": [
                "config = _config.get_config(\"pi0_aloha_sim\")\n",
                "\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_aloha_sim\")\n",
                "key = jax.random.key(0)\n",
                "\n",
                "# Create a model from the checkpoint.\n",
                "model = config.model.load(_model.restore_params(checkpoint_dir / \"params\"))\n",
                "\n",
                "# We can create fake observations and actions to test the model.\n",
                "obs, act = config.model.fake_obs(), config.model.fake_act()\n",
                "\n",
                "# Sample actions from the model.\n",
                "loss = model.compute_loss(key, obs, act)\n",
                "print(\"Loss shape:\", loss.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we are going to create a data loader and use a real batch of training data to compute the loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Column",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m config = dataclasses.replace(config, batch_size=\u001b[32m2\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a single batch of data. This is the same data that will be used during training.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# NOTE: In order to make this example self-contained, we are skipping the normalization step\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# since it requires the normalization statistics to be generated using `compute_norm_stats`.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loader = \u001b[43m_data_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_norm_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m obs, act = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Sample actions from the model.\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/code/openpi/src/openpi/training/data_loader.py:256\u001b[39m, in \u001b[36mcreate_data_loader\u001b[39m\u001b[34m(config, sharding, shuffle, num_batches, skip_norm_stats, framework)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_config.rlds_data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m create_rlds_data_loader(\n\u001b[32m    247\u001b[39m         data_config,\n\u001b[32m    248\u001b[39m         action_horizon=config.model.action_horizon,\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m         framework=framework,\n\u001b[32m    255\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_torch_data_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_horizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_norm_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_norm_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/code/openpi/src/openpi/training/data_loader.py:302\u001b[39m, in \u001b[36mcreate_torch_data_loader\u001b[39m\u001b[34m(data_config, model_config, action_horizon, batch_size, sharding, skip_norm_stats, shuffle, num_batches, num_workers, seed, framework)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_torch_data_loader\u001b[39m(\n\u001b[32m    272\u001b[39m     data_config: _config.DataConfig,\n\u001b[32m    273\u001b[39m     model_config: _model.BaseModelConfig,\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m     framework: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    284\u001b[39m ) -> DataLoader[\u001b[38;5;28mtuple\u001b[39m[_model.Observation, _model.Actions]]:\n\u001b[32m    285\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a data loader for training.\u001b[39;00m\n\u001b[32m    286\u001b[39m \n\u001b[32m    287\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[33;03m        seed: The seed to use for shuffling the data.\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     dataset = \u001b[43mcreate_torch_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_horizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     dataset = transform_dataset(dataset, data_config, skip_norm_stats=skip_norm_stats)\n\u001b[32m    305\u001b[39m     \u001b[38;5;66;03m# Use TorchDataLoader for both frameworks\u001b[39;00m\n\u001b[32m    306\u001b[39m     \u001b[38;5;66;03m# For PyTorch DDP, create DistributedSampler and divide batch size by world size\u001b[39;00m\n\u001b[32m    307\u001b[39m     \u001b[38;5;66;03m# For JAX, divide by process count\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/code/openpi/src/openpi/training/data_loader.py:141\u001b[39m, in \u001b[36mcreate_torch_dataset\u001b[39m\u001b[34m(data_config, action_horizon, model_config)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FakeDataset(model_config, num_samples=\u001b[32m1024\u001b[39m)\n\u001b[32m    140\u001b[39m dataset_meta = lerobot_dataset.LeRobotDatasetMetadata(repo_id)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m dataset = \u001b[43mlerobot_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLeRobotDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maction_horizon\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_sequence_keys\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data_config.prompt_from_task:\n\u001b[32m    149\u001b[39m     dataset = TransformedDataset(dataset, [_transforms.PromptFromLeRobotTask(dataset_meta.tasks)])\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/openpi/lib/python3.11/site-packages/lerobot/common/datasets/lerobot_dataset.py:508\u001b[39m, in \u001b[36mLeRobotDataset.__init__\u001b[39m\u001b[34m(self, repo_id, root, episodes, image_transforms, delta_timestamps, tolerance_s, revision, force_cache_sync, download_videos, video_backend)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mself\u001b[39m.episode_data_index = get_episode_data_index(\u001b[38;5;28mself\u001b[39m.meta.episodes, \u001b[38;5;28mself\u001b[39m.episodes)\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Check timestamps\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m timestamps = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhf_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m    509\u001b[39m episode_indices = torch.stack(\u001b[38;5;28mself\u001b[39m.hf_dataset[\u001b[33m\"\u001b[39m\u001b[33mepisode_index\u001b[39m\u001b[33m\"\u001b[39m]).numpy()\n\u001b[32m    510\u001b[39m ep_data_index_np = {k: t.numpy() \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.episode_data_index.items()}\n",
                        "\u001b[31mTypeError\u001b[39m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Column"
                    ]
                }
            ],
            "source": [
                "# Reduce the batch size to reduce memory usage.\n",
                "config = dataclasses.replace(config, batch_size=2)\n",
                "\n",
                "# Load a single batch of data. This is the same data that will be used during training.\n",
                "# NOTE: In order to make this example self-contained, we are skipping the normalization step\n",
                "# since it requires the normalization statistics to be generated using `compute_norm_stats`.\n",
                "loader = _data_loader.create_data_loader(config, num_batches=1, skip_norm_stats=True)\n",
                "obs, act = next(iter(loader))\n",
                "\n",
                "# Sample actions from the model.\n",
                "loss = model.compute_loss(key, obs, act)\n",
                "\n",
                "# Delete the model to free up memory.\n",
                "del model\n",
                "\n",
                "print(\"Loss shape:\", loss.shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "openpi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
